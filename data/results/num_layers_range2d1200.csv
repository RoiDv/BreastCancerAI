ReLU - Layer #,Accuracy,TP,TN,FP,FN,Sensitivity,Specificity,F1,ROC_AUC
1,0.881578947368421,0.34210526315789475,0.5394736842105263,0.05263157894736842,0.06578947368421052,0.8387096774193549,0.9111111111111112,0.8524590163934426,0.9483870967741935
2,0.9078947368421053,0.34210526315789475,0.5657894736842105,0.02631578947368421,0.06578947368421052,0.8387096774193549,0.9555555555555556,0.8813559322033898,0.9792114695340501
3,0.9210526315789473,0.32894736842105265,0.5921052631578947,0.0,0.07894736842105263,0.8064516129032258,1.0,0.8928571428571428,0.9942652329749104
4,1.0,0.40789473684210525,0.5921052631578947,0.0,0.0,1.0,1.0,1.0,1.0
5,1.0,0.40789473684210525,0.5921052631578947,0.0,0.0,1.0,1.0,1.0,1.0
6,1.0,0.40789473684210525,0.5921052631578947,0.0,0.0,1.0,1.0,1.0,1.0
